<h3 id="philosophy">人工智慧功課<h3>
<h4>目標:利用手寫辨識資料說明MLP 中, 神經網路層的多寡跟預測結果的差異</h4>
一、	數據分析<br>
1. 收斂<br>
#收斂速度：從訓練損失的下降速度來看，增加層數有助於模型更快地收斂。如6層MLP的訓練損失下降(2.29-0.13)得比1層MLP(1.2-0.79)快得多<br>。這可能是因為更深的網絡在初始階段就能捕捉到更複雜的數據表示，從而加速了優化過程。<br>
#	損失曲線的平滑性：比較不同層數的MLP，我們可以觀察到，隨著層數增加，訓練和測試損失的波動性減少。<br>這表示深層網路在學習過程中更為穩定，可能是由於學習到的特徵更加抽象和泛化。<br>
2. 準確率<br>
#過度擬合與測試準確率：在層數增加的情況下，需要關注過度擬合的問題。<br>然而，從這些數據來看，測試準確率持續提升(91%-97%)，這表示模型在這些設置下管理了過度擬合的風險。<br>這可能是因為dropout技術的使用，有效地減少了依賴於特定訓練樣本的風險。<br>
3. 理論<br>
#表示學習理論：從理論上講，增加隱藏層可以讓網絡學習更多層次的抽象，從簡單的形狀到複雜的對象概念。<br>這是深度學習相比於淺層學習的主要優勢，可以提供更豐富的數據表示。<br>
#	萃取能力：更多的層次意味著更高的數據抽象能力，這通常導致更好的特徵萃取，從而對複雜的、非線性的問題具有更強的解決能力。<br>
三、	結論<br>
從上述的圖表中可以發現，增加多個神經網路層（MLP）的層數可顯著提升模型的訓練與測試性能，降低損失並提高準確率。<br>透過合理的設計如使用dropout，即使是深層網路也能有效控制過擬合，增強模型的泛化能力。<br>而選擇適當的層數則需考慮到計算資源與訓練成本，並透過多次的實驗確定最適當的層數。<br>
*github網址: https://github.com/zilingggg/artificial-intelligence.git

