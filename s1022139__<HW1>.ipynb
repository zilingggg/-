{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zilingggg/-artificial-intelligence/blob/main/s1022139__%3CHW1%3E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp7mr90OEXex",
        "outputId": "1d1ab498-f745-4d99-a6fc-f628af9a4181"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 20 04:16:17 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!/usr/local/cuda/bin/nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnRBnE3f7oSI",
        "outputId": "a416d760-63d8-42eb-d472-4a3073cd379c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "drive  sample_data\n",
            " B048D719-C405-4FB9-8555-2C54446970E0.jpeg  'My Drive'\t\t Untitled1.ipynb    未命名文件.gdoc\n",
            "'Colab Notebooks'\t\t\t     MyDrive\t\t yolov4-tiny_1103   照片\n",
            " GoodNotes\t\t\t\t    'python 爬網.gdoc'\t yolov4-tiny_1123   爬網.ipynb\n",
            " mp3\t\t\t\t\t     Untitled0.ipynb\t yolov4-tiny_1214\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!ln -s /content/drive/MyDrive/ /my_drive\n",
        "!ls\n",
        "!ls /my_drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HEo3eJT-L70",
        "outputId": "e3706706-6ef3-497f-a450-ec62b1f5cd4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testing  training  validation\n"
          ]
        }
      ],
      "source": [
        "!ls '/content/drive/MyDrive/Colab Notebooks/food-11'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goMuUyca3NWZ",
        "outputId": "623876e5-8b40-41c5-b22d-85d047db650c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading data--------->\n",
            "Size of training data = 9866\n",
            "Size of validation data = 3430\n",
            "Size of testing data = 3347\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def readfile(path, label):\n",
        "\n",
        "    image_dir = sorted(os.listdir((path)))\n",
        "    x = np.zeros((len(image_dir), 128, 128, 3), dtype=np.uint8)\n",
        "\n",
        "    y = np.zeros((len(image_dir)), dtype=np.uint8)\n",
        "    for i, file in enumerate(image_dir):\n",
        "\n",
        "        img = cv2.imread(os.path.join(path, file))\n",
        "        x[i, :, :, :] = cv2.resize(img, (128,128))\n",
        "        if label:\n",
        "            y[i] = int(file.split(\"_\")[0])\n",
        "\n",
        "    if label:\n",
        "        return x, y\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "workspace_dir = '/content/drive/MyDrive/Colab Notebooks/food-11'\n",
        "print(\"Reading data--------->\")\n",
        "train_x, train_y = readfile(os.path.join(workspace_dir, \"training\"), True)\n",
        "print(\"Size of training data = {}\".format(len(train_x)))\n",
        "val_x, val_y = readfile(os.path.join(workspace_dir, \"validation\"), True)\n",
        "print(f'Size of validation data = {len(val_x)}')\n",
        "test_x = readfile(os.path.join(workspace_dir, \"testing\"), False)\n",
        "print(\"Size of testing data = {}\".format(len(test_x)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcgYp52IBzUS",
        "outputId": "63dbb147-49b4-4d36-e887-a51bea4d68da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loading and preprocessing completed.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, x, y=None, transform=None):\n",
        "        self.x = x\n",
        "\n",
        "        self.y = y\n",
        "        if y is not None:\n",
        "            self.y = torch.LongTensor(y)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "    def __getitem__(self, index):\n",
        "        X = self.x[index]\n",
        "        if self.transform is not None:\n",
        "            X = self.transform(X)\n",
        "        if self.y is not None:\n",
        "            Y = self.y[index]\n",
        "            return X, Y\n",
        "        else:\n",
        "            return X\n",
        "batch_size = 128\n",
        "train_set = ImgDataset(train_x, train_y, train_transform)\n",
        "val_set = ImgDataset(val_x, val_y, test_transform)\n",
        "train_loader = DataLoader(train_set , batch_size= batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
        "print(\"Data loading and preprocessing completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfkzkcJQCVTP"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2,0),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "\n",
        "            nn.Conv2d(512, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2, 0),\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512* 4* 4, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512,11)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = self.cnn(x)\n",
        "        out = out.view(out.size()[0], -1)\n",
        "        return self.fc(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cprAYBJW7c6B",
        "outputId": "1cb290a9-8408-48a0-83a6-6cc4b08e12ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001/045] 25.11 sec(s) Train Acc: 0.244577 Loss: 0.017500 | Val Acc: 0.244315 loss: 0.016401\n",
            "[002/045] 25.19 sec(s) Train Acc: 0.346645 Loss: 0.014673 | Val Acc: 0.266472 loss: 0.018195\n",
            "[003/045] 25.05 sec(s) Train Acc: 0.403000 Loss: 0.013433 | Val Acc: 0.377551 loss: 0.014125\n",
            "[004/045] 25.49 sec(s) Train Acc: 0.453578 Loss: 0.012396 | Val Acc: 0.376385 loss: 0.016624\n",
            "[005/045] 25.11 sec(s) Train Acc: 0.480438 Loss: 0.011764 | Val Acc: 0.500875 loss: 0.011719\n",
            "[006/045] 25.28 sec(s) Train Acc: 0.514798 Loss: 0.010997 | Val Acc: 0.466764 loss: 0.012319\n",
            "[007/045] 24.97 sec(s) Train Acc: 0.545814 Loss: 0.010303 | Val Acc: 0.505539 loss: 0.011503\n",
            "[008/045] 25.07 sec(s) Train Acc: 0.563957 Loss: 0.009942 | Val Acc: 0.503790 loss: 0.011850\n",
            "[009/045] 24.99 sec(s) Train Acc: 0.597203 Loss: 0.009289 | Val Acc: 0.446939 loss: 0.015579\n",
            "[010/045] 25.08 sec(s) Train Acc: 0.622643 Loss: 0.008612 | Val Acc: 0.567055 loss: 0.010164\n",
            "[011/045] 25.00 sec(s) Train Acc: 0.643929 Loss: 0.008011 | Val Acc: 0.584840 loss: 0.009574\n",
            "[012/045] 24.97 sec(s) Train Acc: 0.667038 Loss: 0.007565 | Val Acc: 0.506122 loss: 0.013620\n",
            "[013/045] 25.01 sec(s) Train Acc: 0.666633 Loss: 0.007524 | Val Acc: 0.547813 loss: 0.010968\n",
            "[014/045] 24.90 sec(s) Train Acc: 0.676667 Loss: 0.007329 | Val Acc: 0.573178 loss: 0.010662\n",
            "[015/045] 25.07 sec(s) Train Acc: 0.698155 Loss: 0.006857 | Val Acc: 0.537318 loss: 0.011343\n",
            "[016/045] 25.06 sec(s) Train Acc: 0.719745 Loss: 0.006402 | Val Acc: 0.590087 loss: 0.010445\n",
            "[017/045] 25.34 sec(s) Train Acc: 0.728360 Loss: 0.006066 | Val Acc: 0.564723 loss: 0.011272\n",
            "[018/045] 24.96 sec(s) Train Acc: 0.741131 Loss: 0.005775 | Val Acc: 0.616910 loss: 0.009710\n",
            "[019/045] 24.91 sec(s) Train Acc: 0.752382 Loss: 0.005647 | Val Acc: 0.600875 loss: 0.010108\n",
            "[020/045] 25.09 sec(s) Train Acc: 0.766572 Loss: 0.005285 | Val Acc: 0.604665 loss: 0.010392\n",
            "[021/045] 25.01 sec(s) Train Acc: 0.766977 Loss: 0.005236 | Val Acc: 0.591837 loss: 0.011688\n",
            "[022/045] 24.93 sec(s) Train Acc: 0.783803 Loss: 0.004949 | Val Acc: 0.583965 loss: 0.011531\n",
            "[023/045] 24.89 sec(s) Train Acc: 0.772451 Loss: 0.005173 | Val Acc: 0.639067 loss: 0.009484\n",
            "[024/045] 24.94 sec(s) Train Acc: 0.791303 Loss: 0.004641 | Val Acc: 0.658601 loss: 0.008934\n",
            "[025/045] 24.89 sec(s) Train Acc: 0.810967 Loss: 0.004293 | Val Acc: 0.649854 loss: 0.009429\n",
            "[026/045] 24.85 sec(s) Train Acc: 0.820393 Loss: 0.004137 | Val Acc: 0.619242 loss: 0.011082\n",
            "[027/045] 24.94 sec(s) Train Acc: 0.837726 Loss: 0.003699 | Val Acc: 0.627405 loss: 0.010891\n",
            "[028/045] 25.18 sec(s) Train Acc: 0.836509 Loss: 0.003699 | Val Acc: 0.655102 loss: 0.010466\n",
            "[029/045] 24.93 sec(s) Train Acc: 0.829718 Loss: 0.003908 | Val Acc: 0.574344 loss: 0.015505\n",
            "[030/045] 25.01 sec(s) Train Acc: 0.842388 Loss: 0.003636 | Val Acc: 0.655102 loss: 0.009949\n",
            "[031/045] 24.96 sec(s) Train Acc: 0.872289 Loss: 0.002825 | Val Acc: 0.657726 loss: 0.011087\n",
            "[032/045] 25.00 sec(s) Train Acc: 0.887898 Loss: 0.002627 | Val Acc: 0.672303 loss: 0.009815\n",
            "[033/045] 25.05 sec(s) Train Acc: 0.869045 Loss: 0.002873 | Val Acc: 0.547522 loss: 0.016109\n",
            "[034/045] 25.04 sec(s) Train Acc: 0.886682 Loss: 0.002539 | Val Acc: 0.663265 loss: 0.011160\n",
            "[035/045] 25.08 sec(s) Train Acc: 0.892358 Loss: 0.002399 | Val Acc: 0.674344 loss: 0.011087\n",
            "[036/045] 25.01 sec(s) Train Acc: 0.918711 Loss: 0.001845 | Val Acc: 0.687172 loss: 0.010639\n",
            "[037/045] 24.96 sec(s) Train Acc: 0.919116 Loss: 0.001779 | Val Acc: 0.668222 loss: 0.012372\n",
            "[038/045] 24.81 sec(s) Train Acc: 0.918103 Loss: 0.001869 | Val Acc: 0.670554 loss: 0.011993\n",
            "[039/045] 25.26 sec(s) Train Acc: 0.939286 Loss: 0.001407 | Val Acc: 0.654519 loss: 0.013961\n",
            "[040/045] 24.88 sec(s) Train Acc: 0.916582 Loss: 0.001944 | Val Acc: 0.642857 loss: 0.014423\n",
            "[041/045] 24.94 sec(s) Train Acc: 0.914656 Loss: 0.001927 | Val Acc: 0.639359 loss: 0.014029\n",
            "[042/045] 24.98 sec(s) Train Acc: 0.919015 Loss: 0.001793 | Val Acc: 0.683673 loss: 0.012556\n",
            "[043/045] 24.97 sec(s) Train Acc: 0.951145 Loss: 0.001253 | Val Acc: 0.665015 loss: 0.013810\n",
            "[044/045] 24.97 sec(s) Train Acc: 0.897426 Loss: 0.002437 | Val Acc: 0.663557 loss: 0.013137\n",
            "[045/045] 24.96 sec(s) Train Acc: 0.948814 Loss: 0.001209 | Val Acc: 0.639067 loss: 0.016029\n"
          ]
        }
      ],
      "source": [
        "model = Classifier()\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "if torch.cuda.is_available():\n",
        "    loss = loss.cuda()\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "num_epoch = 45\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    model.train()\n",
        "    for data in train_loader:\n",
        "        img,targets = data\n",
        "        if torch.cuda.is_available():\n",
        "            img = img.cuda()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        train_pred = model(img)\n",
        "\n",
        "        batch_loss = loss(train_pred,targets)\n",
        "\n",
        "        batch_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc += (train_pred.argmax(1)==targets).sum()\n",
        "        train_loss += batch_loss.item()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            img, targets = data\n",
        "            if torch.cuda.is_available():\n",
        "                img = img.cuda()\n",
        "                targets = targets.cuda()\n",
        "            val_pred = model(img)\n",
        "            batch_loss = loss(val_pred,targets)\n",
        "\n",
        "            val_acc += (val_pred.argmax(1) == targets).sum()\n",
        "            val_loss += batch_loss.item()\n",
        "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % (epoch + 1, num_epoch, time.time() - epoch_start_time, train_acc / train_set.__len__(), train_loss / train_set.__len__(), val_acc / val_set.__len__(),val_loss / val_set.__len__()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dtcszDEl5JG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cd74b7-4c7f-43ad-fab9-77e6108ba3fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[001/045] 29.98 sec(s) Train Acc: 0.281588 Loss: 0.016370\n",
            "[002/045] 29.92 sec(s) Train Acc: 0.385304 Loss: 0.013756\n",
            "[003/045] 29.77 sec(s) Train Acc: 0.441712 Loss: 0.012572\n",
            "[004/045] 30.05 sec(s) Train Acc: 0.485259 Loss: 0.011577\n",
            "[005/045] 29.91 sec(s) Train Acc: 0.535349 Loss: 0.010466\n",
            "[006/045] 30.08 sec(s) Train Acc: 0.573782 Loss: 0.009671\n",
            "[007/045] 29.90 sec(s) Train Acc: 0.596646 Loss: 0.009017\n",
            "[008/045] 30.23 sec(s) Train Acc: 0.626730 Loss: 0.008362\n",
            "[009/045] 30.06 sec(s) Train Acc: 0.654257 Loss: 0.007741\n",
            "[010/045] 29.71 sec(s) Train Acc: 0.669073 Loss: 0.007385\n",
            "[011/045] 29.73 sec(s) Train Acc: 0.686673 Loss: 0.007007\n",
            "[012/045] 30.02 sec(s) Train Acc: 0.711567 Loss: 0.006451\n",
            "[013/045] 29.83 sec(s) Train Acc: 0.728640 Loss: 0.006141\n",
            "[014/045] 29.84 sec(s) Train Acc: 0.744058 Loss: 0.005718\n",
            "[015/045] 30.03 sec(s) Train Acc: 0.764064 Loss: 0.005364\n",
            "[016/045] 29.90 sec(s) Train Acc: 0.767223 Loss: 0.005172\n",
            "[017/045] 30.17 sec(s) Train Acc: 0.778354 Loss: 0.004958\n",
            "[018/045] 29.94 sec(s) Train Acc: 0.803625 Loss: 0.004457\n",
            "[019/045] 30.10 sec(s) Train Acc: 0.810695 Loss: 0.004245\n",
            "[020/045] 29.77 sec(s) Train Acc: 0.819720 Loss: 0.003989\n",
            "[021/045] 29.82 sec(s) Train Acc: 0.837996 Loss: 0.003651\n",
            "[022/045] 29.72 sec(s) Train Acc: 0.844088 Loss: 0.003510\n",
            "[023/045] 29.61 sec(s) Train Acc: 0.849729 Loss: 0.003379\n",
            "[024/045] 29.53 sec(s) Train Acc: 0.872292 Loss: 0.002834\n",
            "[025/045] 29.61 sec(s) Train Acc: 0.876203 Loss: 0.002713\n",
            "[026/045] 29.69 sec(s) Train Acc: 0.880716 Loss: 0.002588\n",
            "[027/045] 29.73 sec(s) Train Acc: 0.882897 Loss: 0.002565\n",
            "[028/045] 29.48 sec(s) Train Acc: 0.892825 Loss: 0.002343\n",
            "[029/045] 29.66 sec(s) Train Acc: 0.912981 Loss: 0.001964\n",
            "[030/045] 29.54 sec(s) Train Acc: 0.916366 Loss: 0.001800\n",
            "[031/045] 29.60 sec(s) Train Acc: 0.924338 Loss: 0.001680\n",
            "[032/045] 29.54 sec(s) Train Acc: 0.929377 Loss: 0.001622\n",
            "[033/045] 29.72 sec(s) Train Acc: 0.930054 Loss: 0.001540\n",
            "[034/045] 29.54 sec(s) Train Acc: 0.935469 Loss: 0.001432\n",
            "[035/045] 29.60 sec(s) Train Acc: 0.936221 Loss: 0.001419\n",
            "[036/045] 30.09 sec(s) Train Acc: 0.948631 Loss: 0.001164\n",
            "[037/045] 29.47 sec(s) Train Acc: 0.945623 Loss: 0.001231\n",
            "[038/045] 29.55 sec(s) Train Acc: 0.950587 Loss: 0.001093\n",
            "[039/045] 29.73 sec(s) Train Acc: 0.951188 Loss: 0.001125\n",
            "[040/045] 29.52 sec(s) Train Acc: 0.952843 Loss: 0.001069\n",
            "[041/045] 29.45 sec(s) Train Acc: 0.959687 Loss: 0.000917\n",
            "[042/045] 29.56 sec(s) Train Acc: 0.963673 Loss: 0.000839\n",
            "[043/045] 29.76 sec(s) Train Acc: 0.966682 Loss: 0.000720\n",
            "[044/045] 29.46 sec(s) Train Acc: 0.959462 Loss: 0.000919\n",
            "[045/045] 29.52 sec(s) Train Acc: 0.964801 Loss: 0.000827\n"
          ]
        }
      ],
      "source": [
        "train_val_x = np.concatenate((train_x,val_x),axis=0)\n",
        "train_val_y = np.concatenate((train_y,val_y),axis=0)\n",
        "\n",
        "train_val_set = ImgDataset(train_val_x,train_val_y,train_transform)\n",
        "train_val_lodaer = DataLoader(train_val_set,batch_size=batch_size,shuffle=True)\n",
        "\n",
        "\n",
        "model_best = Classifier()\n",
        "if torch.cuda.is_available():\n",
        "    model_best = model_best.cuda()\n",
        "\n",
        "loss = nn.CrossEntropyLoss()\n",
        "if torch.cuda.is_available():\n",
        "    loss = loss.cuda()\n",
        "\n",
        "optimizer = torch.optim.Adam(model_best.parameters(),lr=0.001)\n",
        "\n",
        "num_epoch = 45\n",
        "for epoch in range(num_epoch):\n",
        "    epoch_start_time = time.time()\n",
        "    train_acc = 0.0\n",
        "    train_loss = 0.0\n",
        "    for data in train_val_lodaer:\n",
        "        img,targets = data\n",
        "        if torch.cuda.is_available():\n",
        "            img = img.cuda()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        train_pred = model_best(img)\n",
        "        batch_loss = loss(train_pred,targets)\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_acc += (train_pred.argmax(1) == targets).sum()\n",
        "        train_loss += batch_loss.item()\n",
        "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % (epoch + 1, num_epoch, time.time()-epoch_start_time, train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 將模型移動到GPU上\n",
        "model_best = model_best.cuda()\n",
        "\n",
        "# 將輸入數據轉換為PyTorch張量，並將其移動到GPU上\n",
        "test_set = ImgDataset(torch.tensor(test_x).cuda(), transform=test_transform\n"
      ],
      "metadata": {
        "id": "ewesIMqHauhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA_QifO6ltUE"
      },
      "outputs": [],
      "source": [
        "test_set = ImgDataset(test_x, transform=test_transform)\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
        "model_best.eval()\n",
        "prediction = []\n",
        "with torch.no_grad():\n",
        "    for i, data in enumerate(test_loader):\n",
        "        test_pred = model_best(data.cuda())\n",
        "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
        "        for y in test_label:\n",
        "            prediction.append(y)\n",
        "#將結果寫入 csv 檔\n",
        "with open(\"predict.csv\", 'w') as f:\n",
        "    f.write('Id,Category\\n')\n",
        "    for i, y in  enumerate(prediction):\n",
        "        f.write('{},{}\\n'.format(i, y))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}